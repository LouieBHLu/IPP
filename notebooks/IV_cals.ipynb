{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature evaluation funcs\n",
    "\n",
    "* coverage\n",
    "* IV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "import bisect\n",
    "import random\n",
    "import math\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numexpr as ne\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "\n",
    "def _missing_value(data, columns, continuousDomain, categoricalDomain):\n",
    "    if columns in continuousDomain:\n",
    "        col = data[columns]\n",
    "        mis_val = ne.evaluate(\"min(col)\") - 1\n",
    "    elif columns in categoricalDomain:\n",
    "        mis_val = 'NA'\n",
    "    else:\n",
    "        raise NameError('columns %s is not in the feature set' % columns)\n",
    "    return {columns: mis_val}\n",
    "\n",
    "\n",
    "def _getTrainQuantile(df, columns, fillna=-1, step=0.1):\n",
    "    length = [x + 1 for x in range(int(1 / step))]\n",
    "\n",
    "    if length[-1] < 1:\n",
    "        length += [1]\n",
    "\n",
    "    q = [x * 0.1 for x in length]\n",
    "    quantile = [fillna] + [x for x in df[columns].quantile(q).values]\n",
    "    return {columns: quantile}\n",
    "\n",
    "\n",
    "def _getBins(data, columns, quantile, fillna=-1):\n",
    "    data['group'] = np.searchsorted(quantile, data[columns].fillna(fillna))\n",
    "    return data\n",
    "\n",
    "\n",
    "def _getPercent(df, columns):\n",
    "    length = float(len(df))\n",
    "    value_set = (df['group'].value_counts() / length).to_dict()\n",
    "    return {columns: value_set}\n",
    "\n",
    "\n",
    "def _getPercentCategorical(df, columns, fillna='NA', dataset='train', train=None):\n",
    "    if dataset == 'test' and train is None:\n",
    "        raise ValueError('Missing train set distribution.')\n",
    "    n = df[columns].shape[0]\n",
    "    df = (df[columns].fillna(fillna).value_counts() / float(n)).to_dict()\n",
    "    return {columns: df}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "\n",
    "class WoE:\n",
    "    \"\"\"\n",
    "    Basic functionality for WoE bucketing of continuous and discrete variables\n",
    "    :param self.bins: DataFrame WoE transformed variable and all related statistics\n",
    "    :param self.iv: Information Value of the transformed variable\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, qnt_num=16, min_block_size=16, spec_values=None,\n",
    "                 v_type='c', bins=None, t_type='b'):\n",
    "        \"\"\"\n",
    "        :param qnt_num: Number of buckets (quartiles) for continuous variable split\n",
    "        :param min_block_size: minimum number of observation in each bucket (\n",
    "                                continuous variables), incl. optimization restrictions\n",
    "        :param spec_values: List or Dictionary {'label': value} of special values (\n",
    "                                frequent items etc.)\n",
    "        :param v_type: 'c' for continuous variable, 'd' - for discrete\n",
    "        :param bins: Predefined bucket borders for continuous variable split\n",
    "        :t_type : Binary 'b' or continous 'c' target variable\n",
    "        :return: initialized class\n",
    "        \"\"\"\n",
    "        self.__qnt_num = qnt_num  # Num of buckets/quartiles\n",
    "        self._predefined_bins = None if bins is None else np.array(bins) if type(\n",
    "            bins) == list else bins  # user bins for continuous variables\n",
    "        self.type = v_type  # if 'c' variable should be continuous, if 'd' - discrete\n",
    "        self._min_block_size = min_block_size  # Min num of observation in bucket\n",
    "        self._gb_ratio = None  # Ratio of good and bad in the sample\n",
    "        self.bins = None  # WoE Buckets (bins) and related statistics\n",
    "        self.d_bins = None  # bins for discrete values\n",
    "        self.c_bins = None\n",
    "        self.sp_bins = None\n",
    "        self.df = None  # Training sample DataFrame with initial data and assigned woe\n",
    "        self.df_sp_values = None\n",
    "        self.df_disc = None\n",
    "        self.df_cont = None\n",
    "        self.qnt_num = None  # Number of quartiles used for continuous part of variable binning\n",
    "        self.t_type = t_type  # Type of target variable\n",
    "        self.mis_val = {}\n",
    "        if type(spec_values) == dict:  # Parsing special values to dict for cont variables\n",
    "            self.spec_values = {}\n",
    "            for k, v in spec_values.items():\n",
    "                if v.startswith('d_'):\n",
    "                    self.spec_values[k] = v\n",
    "                else:\n",
    "                    self.spec_values[k] = 'd_' + v\n",
    "        else:\n",
    "            if spec_values is None:\n",
    "                self.spec_values = {}\n",
    "            else:\n",
    "                self.spec_values = {i: 'd_' + str(i) for i in spec_values}\n",
    "\n",
    "    # Reading data to DataFrame, init DataFrame and spec value preprocessing\n",
    "    def read_data(self, x, y):\n",
    "        # Data quality checks\n",
    "        try:\n",
    "            x = pd.Series(column_or_1d(x)).reset_index(drop=True)\n",
    "        except Exception as e:\n",
    "            raise ValueError('1 dimension data should be input')\n",
    "        y = pd.Series(y).reset_index(drop=True)\n",
    "\n",
    "        if not x.size == y.size:\n",
    "            raise Exception(\"Y size doesn't match X size\")\n",
    "\n",
    "        # Calc total good bad ratio in the sample\n",
    "        t_bad = np.sum(y)\n",
    "        if t_bad == 0 or t_bad == y.size:\n",
    "            raise ValueError(\n",
    "                \"There should be BAD and GOOD observations in the sample\")\n",
    "\n",
    "        if np.max(y) > 1 or np.min(y) < 0:\n",
    "            raise ValueError(\"Y range should be between 0 and 1\")\n",
    "\n",
    "        # Make data frame for calculations\n",
    "        self.df = pd.DataFrame({\"X\": x, \"Y\": y, 'order': np.arange(x.size)})\n",
    "        self.df['labels'] = self.df['X'].apply(lambda x: '0')\n",
    "        # difference of label and labels:label_show &label used\n",
    "        self.df['label'] = self.df['labels']\n",
    "        sp_values_flag = self.df['X'].isin(self.spec_values.keys(\n",
    "        )).values | self.df['X'].isnull().values | (self.df['X'] == -1).values\n",
    "        self.df_sp_values = self.df[sp_values_flag].copy()\n",
    "        self.df_cont = self.df[np.logical_not(sp_values_flag)].copy()\n",
    "        if self.df_cont['X'].unique().size == 1:\n",
    "            self.spec_values.update({self.df_cont['X'].unique()[0]:\n",
    "                                     'd_' + str(self.df_cont['X'].unique()[0])\n",
    "                                     if self.type == 'd' else '0'})\n",
    "        sp_values_flag = self.df['X'].isin(self.spec_values.keys(\n",
    "        )).values | self.df['X'].isnull().values | (self.df['X'] == -1).values\n",
    "        self.df_sp_values = self.df[sp_values_flag].copy()\n",
    "        self.df_cont = self.df[np.logical_not(sp_values_flag)].copy()\n",
    "\n",
    "        self.df_sp_values, sp_bins = self._disc_labels(self.df_sp_values)\n",
    "        self.sp_bins = sp_bins\n",
    "        return self\n",
    "\n",
    "    def fit(self, x=None, y=None):\n",
    "        \"\"\"\n",
    "        Fit WoE transformation\n",
    "        :param x: continuous or discrete predictor\n",
    "        :param y: binary target variable\n",
    "        :return: WoE class\n",
    "        \"\"\"\n",
    "        # 当fit不是在optimize中被调用时：\n",
    "        if x is not None:\n",
    "            self.read_data(x, y)\n",
    "            if self.type == 'd':\n",
    "                if len(self.df_cont['X'].unique()) + len(self.sp_bins) > 100:\n",
    "                    self.optimize()\n",
    "                    return self\n",
    "\n",
    "            if len(self.df_cont) == 0:\n",
    "                self.df_cont = None\n",
    "            self.df_disc, self.df_cont = self._split_sample(self.df_cont)\n",
    "            # # labeling data\n",
    "\n",
    "            df_disc, d_bins = self._disc_labels(self.df_disc)\n",
    "            self.df_disc = df_disc if d_bins is not None else self.df_disc\n",
    "            self.d_bins = d_bins if d_bins is not None else self.d_bins\n",
    "            df_cont, c_bins = self._cont_labels(self.df_cont)\n",
    "            self.df_cont = df_cont if c_bins is not None else self.df_cont\n",
    "            self.c_bins = c_bins if c_bins is not None else self.c_bins\n",
    "\n",
    "        # getting continuous and discrete values together\n",
    "        self.df = self.df_sp_values.append(self.df_cont)\n",
    "        self.df = self.df.append(self.df_disc)\n",
    "        self.bins = self.sp_bins.append(self.c_bins)\n",
    "        self.bins = self.bins.append(self.d_bins)\n",
    "        # calculating woe and other statistics\n",
    "        self._calc_stat()\n",
    "        # sorting appropriately for further cutting in transform method\n",
    "        self.bins.sort_values('bins', inplace=True)\n",
    "        # returning to original observation order\n",
    "        self.df.sort_values('order', inplace=True)\n",
    "        self.df.set_index(self.df['X'].index, inplace=True)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, x, y):\n",
    "        \"\"\"\n",
    "        Fit WoE transformation\n",
    "        :param x: continuous or discrete predictor\n",
    "        :param y: binary target variable\n",
    "        :return: WoE transformed variable\n",
    "        \"\"\"\n",
    "        self.fit(x, y)\n",
    "        return self.df['woe']\n",
    "\n",
    "    def _split_sample(self, df):\n",
    "        if self.type == 'd':\n",
    "            return df, None\n",
    "        else:\n",
    "            return None, df\n",
    "\n",
    "    def _disc_labels(self, df):\n",
    "        if df is None:\n",
    "            return None, None\n",
    "        df['labels'] = df['X'].apply(\n",
    "            lambda x: self.spec_values[x] if x in self.spec_values.keys() else 'd_' + str(x))\n",
    "        df['label'] = df['X'].apply(\n",
    "            lambda x: self.spec_values[x] if x in self.spec_values.keys() else 'd_' + str(x))\n",
    "        d_bins = pd.DataFrame({\"bins\": df['X'].unique()})\n",
    "        d_bins['labels'] = d_bins['bins'].apply(\n",
    "            lambda x: self.spec_values[x] if x in self.spec_values.keys() else 'd_' + str(x))\n",
    "        d_bins['label'] = d_bins['bins'].apply(\n",
    "            lambda x: self.spec_values[x] if x in self.spec_values.keys() else 'd_' + str(x))\n",
    "        return df, d_bins\n",
    "\n",
    "    def _cont_labels(self, df):\n",
    "        # check whether there is a continuous part\n",
    "        if df is None:\n",
    "            return None, None\n",
    "        # Max buckets num calc\n",
    "        self.qnt_num = int(np.minimum(\n",
    "            df['X'].size / self._min_block_size, self.__qnt_num)) + 1\n",
    "        # cuts - label num for each observation, bins - quartile thresholds\n",
    "        bins = None\n",
    "        cuts = None\n",
    "\n",
    "        if self._predefined_bins is None:\n",
    "            try:\n",
    "                cuts, bins = pd.qcut(\n",
    "                    df[\"X\"], self.qnt_num, retbins=True, labels=False)\n",
    "                bins = np.append((min(self.df_cont['X']) - 1,), bins[1:-1])\n",
    "                cuts_show = pd.cut(df['X'].copy(), bins=np.append(\n",
    "                    bins, (max(self.df_cont['X']),)))\n",
    "            except ValueError as ex:\n",
    "\n",
    "                if ex.args[0].startswith('Bin edges must be unique'):\n",
    "                    self.optimize()\n",
    "                    return None, None\n",
    "                    ex.args = (\n",
    "                        'Please reduce number of bins or encode\\\n",
    "                        frequent items as special values',) + ex.args\n",
    "                    raise\n",
    "        else:\n",
    "            bins = self._predefined_bins\n",
    "            if bins[0] != min(self.df_cont['X']) - 1:\n",
    "                bins = np.append((min(self.df_cont['X']) - 1,), bins)\n",
    "            cuts = pd.cut(df['X'], bins=np.append(bins, (max(self.df_cont['X']),)),\n",
    "                          labels=np.arange(len(bins)).astype(str))\n",
    "            cuts_show = pd.cut(df['X'], bins=np.append(\n",
    "                bins, (max(self.df_cont['X']),)))\n",
    "        df[\"labels\"] = cuts.astype(str)\n",
    "        df['label'] = cuts_show.astype(str)\n",
    "\n",
    "        c_bins = pd.DataFrame(\n",
    "            {\"bins\": bins, \"labels\": np.arange(len(bins)).astype(str),\n",
    "             'label': cuts_show.cat.categories})\n",
    "        self.df_cont, self.c_bins = df, c_bins\n",
    "        return df, c_bins\n",
    "\n",
    "    # calculating woe and other statistics in this part\n",
    "    def _calc_stat(self):\n",
    "        # calculating WoE\n",
    "        col_names = {'count_nonzero': 'bad', 'size': 'obs'}\n",
    "        stat = self.df.groupby(\"labels\")['Y'].agg(\n",
    "            [np.mean, np.count_nonzero, np.size]).rename(columns=col_names).copy()\n",
    "        if self.t_type != 'b':\n",
    "            stat['bad'] = stat['mean'] * stat['obs']\n",
    "        stat['good'] = stat['obs'] - stat['bad']\n",
    "        t_good = stat['good'].sum()\n",
    "        t_bad = stat['bad'].sum()\n",
    "\n",
    "        iv_calc = stat.copy()\n",
    "        iv_calc['pg'] = iv_calc['good'].apply(\n",
    "            lambda x: x / t_good if x != 0 else 1 / (t_good + len(iv_calc)))\n",
    "        iv_calc['pb'] = iv_calc['bad'].apply(\n",
    "            lambda x: x / t_bad if x != 0 else 1 / (t_good + len(iv_calc)))\n",
    "        iv_calc['odds_prob'] = iv_calc['pb'] / iv_calc['pg']\n",
    "        iv_calc['woe'] = iv_calc['odds_prob'].apply(lambda x: np.log(x))\n",
    "        iv_calc['iv'] = (iv_calc['pb'] - iv_calc['pg']) * iv_calc['woe']\n",
    "        stat['odds'] = stat['good'] / stat['bad']\n",
    "        stat['woe'] = iv_calc['woe']\n",
    "        iv_stat = {'labels': iv_calc.index.values, 'iv': iv_calc['iv'].values}\n",
    "\n",
    "        iv_stat = pd.DataFrame(data=iv_stat)\n",
    "        self.iv_stat = iv_stat\n",
    "\n",
    "        self.iv = iv_stat['iv'].sum()\n",
    "        # adding stat data to bins\n",
    "        self.bins = pd.merge(\n",
    "            stat, self.bins, left_index=True, right_on=['labels'])\n",
    "        self.bins = pd.merge(self.bins, iv_stat, on=['labels'])\n",
    "        label_woe = self.bins[['woe', 'labels',\n",
    "                               'label']].drop_duplicates()\n",
    "        self.df = pd.merge(self.df, label_woe, left_on=[\n",
    "                           'labels'], right_on=['labels'])\n",
    "\n",
    "    def transform(self, x, manual_woe=None):\n",
    "        \"\"\"\n",
    "        Transforms input variable according to previously fitted rule\n",
    "        :param x: input variable\n",
    "        :param manual_woe: one can change fitted woe with manual\n",
    "                            values by providing dict {label: new_woe_value}\n",
    "        :return: DataFrame with transformed with original and transformed variables\n",
    "        \"\"\"\n",
    "        if not isinstance(x, pd.Series):\n",
    "            raise TypeError(\"pandas.Series type expected\")\n",
    "\n",
    "        if self.bins is None:\n",
    "            raise Exception('Fit the model first, please')\n",
    "        df = pd.DataFrame({\"X\": x, 'order': np.arange(x.size)})\n",
    "        # splitting to discrete and continous pars\n",
    "        df_sp_values, df_cont = self._split_sample(df)\n",
    "\n",
    "        # Replacing original with manual woe\n",
    "        if manual_woe:\n",
    "            tr_bins = self.bins[['woe', 'labels', 'label']].copy()\n",
    "            if not type(manual_woe) == dict:\n",
    "                TypeError(\"manual_woe should be dict\")\n",
    "            else:\n",
    "                for key in manual_woe:\n",
    "                    tr_bins['woe'].mask(tr_bins['labels'] ==\n",
    "                                        key, manual_woe[key], inplace=True)\n",
    "        else:\n",
    "            tr_bins = self.bins\n",
    "            # function checks existence of special values,\n",
    "            # raises error if sp do not exist in training set\n",
    "\n",
    "        def get_sp_label(x_):\n",
    "            if x_ in self.spec_values.keys():\n",
    "                return self.spec_values[x_]\n",
    "            else:\n",
    "                str_x = 'd_' + str(x_)\n",
    "                if str_x in list(self.bins['labels']):\n",
    "                    return str_x\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        'Value {} does not exist in the training set'.format(str_x))\n",
    "\n",
    "        # assigning labels to discrete part\n",
    "        df_sp_values['labels'] = df_sp_values['X'].apply(get_sp_label)\n",
    "        df_sp_values['label'] = df_sp_values['X'].apply(get_sp_label)\n",
    "        # assigning labels to continuous part\n",
    "        c_bins = self.bins[self.bins['labels'].apply(\n",
    "            lambda z: not z.startswith('d_'))]\n",
    "\n",
    "        if not self.type == 'd':\n",
    "            # if bins[0] != min(self.df['X']) - 1:\n",
    "            #     bins = np.append((min(self.df['X']) - 1,), bins)\n",
    "            cuts = pd.cut(df_cont['X'], bins=np.append(\n",
    "                c_bins[\"bins\"], (max(self.df_cont['X']),)), labels=c_bins[\"labels\"])\n",
    "            cuts_show = pd.cut(df_cont['X'], bins=np.append(\n",
    "                c_bins[\"bins\"], (max(self.df_cont['X']),)))\n",
    "            df_cont['labels'] = cuts.astype(str)\n",
    "            df_cont['label'] = cuts_show.astype(str)\n",
    "        # Joining continuous and discrete parts\n",
    "        df = df_sp_values.append(df_cont)\n",
    "\n",
    "        # assigning woe\n",
    "        df = pd.merge(df, tr_bins[['woe', 'labels', 'label']].drop_duplicates(), left_on=[\n",
    "                      'labels'], right_on=['labels'])\n",
    "        # returning to original observation order\n",
    "        df.sort_values('order', inplace=True)\n",
    "        return df.set_index(x.index)\n",
    "\n",
    "    def merge(self, label1, label2=None):\n",
    "        \"\"\"\n",
    "        Merge of buckets with given labels\n",
    "        In case of discrete variable, both labels should be provided.\n",
    "                    As the result labels will be marget to one bucket.\n",
    "        In case of continous variable, only label1 should be provided.\n",
    "                    It will be merged with the next label.\n",
    "        :param label1: first label to merge\n",
    "        :param label2: second label to merge\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        spec_values = self.spec_values.copy()\n",
    "        c_bins = self.bins[self.bins['labels'].apply(\n",
    "            lambda x: not x.startswith('d_'))].copy()\n",
    "        # removing bucket for continuous variable\n",
    "        if label2 is None and not label1.startswith('d_'):\n",
    "            c_bins = c_bins[c_bins['labels'] != label1]\n",
    "        else:\n",
    "            if not (label1.startswith('d_') and label2.startswith('d_')):\n",
    "                raise Exception('Labels should be discrete simultaneously')\n",
    "            for i in self.bins[self.bins['labels'] == label1]['bins']:\n",
    "                spec_values[i] = label1 + '_' + label2\n",
    "            bin2 = self.bins[self.bins['labels'] == label2]['bins'].iloc[0]\n",
    "            spec_values[bin2] = label1 + '_' + label2\n",
    "        new_woe = WoE(self.__qnt_num, self._min_block_size,\n",
    "                      spec_values, self.type, c_bins['bins'], self.t_type)\n",
    "        return new_woe.fit(self.df['X'], self.df['Y'])\n",
    "\n",
    "    def plot(self, sort_values=True, labels=False):\n",
    "        \"\"\"\n",
    "        Plot WoE transformation and default rates\n",
    "        :param sort_values: whether to sort discrete variables by woe, continuous by labels\n",
    "        :param labels: plot labels or intervals for continuous buckets\n",
    "        :return: plotting object\n",
    "        \"\"\"\n",
    "        bar_width = 0.8\n",
    "        woe_fig = plt.figure()\n",
    "        plt.title('Number of Observations and WoE per bucket')\n",
    "        ax = woe_fig.add_subplot(111)\n",
    "        ax.set_ylabel('Observations')\n",
    "        plot_data = self.bins[['labels', 'woe',\n",
    "                               'obs', 'bins']].copy().drop_duplicates()\n",
    "\n",
    "        if sort_values:\n",
    "            if self.type == 'd':\n",
    "                plot_data.sort_values('woe', inplace=True)\n",
    "            else:\n",
    "                cont_labels = plot_data['labels'].apply(\n",
    "                    lambda z: not z.startswith('d_'))\n",
    "                temp_data = plot_data[cont_labels].sort_values('bins')\n",
    "                plot_data = temp_data.append(\n",
    "                    plot_data[~cont_labels].sort_values('labels'))\n",
    "        # creating plot labels\n",
    "        plot_data['plot_bins'] = plot_data['bins'].apply(\n",
    "            lambda x: '{}'.format(x))\n",
    "        if not self.type == 'd':\n",
    "            right_bound = plot_data['plot_bins'].iloc[1:].append(\n",
    "                pd.Series([str(max(self.df_cont['X']))]))\n",
    "            plot_data['plot_bins'] = plot_data['plot_bins'].add(' : ')\n",
    "            plot_data['plot_bins'] = plot_data['plot_bins'].add(\n",
    "                list(right_bound))\n",
    "        cont_labels = plot_data['labels'].apply(\n",
    "            lambda z: not z.startswith('d_'))\n",
    "        plot_data['plot_bins'] = np.where(\n",
    "            cont_labels, plot_data['plot_bins'], plot_data['labels'])\n",
    "        # start plotting\n",
    "        index = np.arange(plot_data.shape[0])\n",
    "        plt.xticks(index + 0.8 * bar_width,\n",
    "                   plot_data['labels'] if labels else plot_data['plot_bins'])\n",
    "        plt.bar(index, plot_data['obs'], bar_width,\n",
    "                color='b', label='Observations')\n",
    "        ax2 = ax.twinx()\n",
    "        ax2.set_ylabel('Weight of Evidence')\n",
    "        ax2.plot(index + bar_width / 2,\n",
    "                 plot_data['woe'], 'bo-', linewidth=4.0, color='r', label='WoE')\n",
    "        handles1, labels1 = ax.get_legend_handles_labels()\n",
    "        handles2, labels2 = ax2.get_legend_handles_labels()\n",
    "        handles = handles1 + handles2\n",
    "        labels = labels1 + labels2\n",
    "        plt.legend(handles, labels)\n",
    "        woe_fig.autofmt_xdate()\n",
    "        return woe_fig\n",
    "\n",
    "    def new_bin(self, criterion='gini', fix_depth=None, max_depth=None,\n",
    "                cv=3, scoring=None, min_samples_leaf=None):\n",
    "        \"\"\"\n",
    "        WoE bucketing optimization (continuous variables only)\n",
    "        :param criterion: binary tree split criteria\n",
    "        :param fix_depth: use tree of a fixed depth (2^fix_depth buckets)\n",
    "        :param max_depth: maximum tree depth for a optimum cross-validation search\n",
    "        :param cv: number of cv buckets\n",
    "        :param scoring: scorer for cross_val_score\n",
    "        :param min_samples_leaf: minimum number of observations in each of optimized buckets\n",
    "        :return: WoE class with optimized continuous variable split\n",
    "        \"\"\"\n",
    "        if self.t_type == 'b':\n",
    "            tree_type = tree.DecisionTreeClassifier\n",
    "        else:\n",
    "            tree_type = tree.DecisionTreeRegressor\n",
    "\n",
    "        m_depth = int(np.log2(self.__qnt_num)) + 1 if max_depth is None else max_depth\n",
    "        min_samples_leaf = self._min_block_size if min_samples_leaf is None else min_samples_leaf\n",
    "        cont = self.df_cont\n",
    "        if cont is None:\n",
    "            return None, None\n",
    "        if cont.empty:\n",
    "            return None, None\n",
    "\n",
    "        x_train = cont['X']\n",
    "        y_train = cont['Y']\n",
    "        x_train = x_train.values.reshape(x_train.shape[0], 1)\n",
    "        start = 1\n",
    "        cv_scores = []\n",
    "\n",
    "        if fix_depth is None:\n",
    "            for i in range(start, m_depth):\n",
    "                d_tree = tree_type(\n",
    "                    criterion=criterion, max_depth=i, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "                if self.type == 'd':\n",
    "                    scores = cross_val_score(d_tree, pd.get_dummies(\n",
    "                        cont['X']), cont['Y'], cv=cv, scoring=scoring)\n",
    "                else:\n",
    "                    scores = cross_val_score(\n",
    "                        d_tree, x_train, y_train, cv=cv, scoring=scoring)\n",
    "                cv_scores.append(scores.mean())\n",
    "            best = np.argmax(cv_scores) + start\n",
    "        else:\n",
    "            best = fix_depth\n",
    "\n",
    "        final_tree = tree_type(\n",
    "            criterion=criterion, max_depth=best, min_samples_leaf=min_samples_leaf)\n",
    "\n",
    "        if self.type == 'd':\n",
    "            final_tree.fit(pd.get_dummies(cont['X']), cont['Y'])\n",
    "        else:\n",
    "            final_tree.fit(x_train, y_train)\n",
    "\n",
    "        '''\n",
    "        print(final_tree.tree_.feature)\n",
    "        from sklearn.tree import export_graphviz\n",
    "        import os\n",
    "        export_graphviz(final_tree)\n",
    "        os.system('dot -Tpng tree.dot -o tree.png')\n",
    "        '''\n",
    "\n",
    "        # for categorical feature's optimization\n",
    "        if self.type == 'd':\n",
    "            opt_bins = final_tree.tree_.feature[final_tree.tree_.feature >= 0]\n",
    "            cat_f_sort = sorted(cont['X'].unique())\n",
    "            bins = self.d_bins['bins'].as_matrix().tolist(\n",
    "            ) if self.d_bins is not None else []\n",
    "\n",
    "            for item in opt_bins:\n",
    "                bins.append(cat_f_sort[item])\n",
    "            cat_f_new = self.df['X'].apply(\n",
    "                lambda x: \"others\" if x not in bins else x)\n",
    "\n",
    "            return cat_f_new, bins\n",
    "\n",
    "        # for continuous feature's optimization\n",
    "        opt_bins = np.sort(\n",
    "            final_tree.tree_.threshold[final_tree.tree_.feature >= 0])\n",
    "        if len(opt_bins) == 0:\n",
    "            opt_bins = np.append((min(self.df_cont['X']) - 1,), opt_bins)\n",
    "        if opt_bins[0] != min(self.df_cont['X']) - 1:\n",
    "            opt_bins = np.append((min(self.df_cont['X']) - 1,), opt_bins)\n",
    "        cuts = pd.cut(cont['X'], bins=np.append(opt_bins, (max(self.df_cont['X']),)),\n",
    "                      labels=np.arange(len(opt_bins)).astype(str))\n",
    "        cuts_show = pd.cut(cont['X'], np.append(\n",
    "            opt_bins, (max(self.df_cont['X']),)))\n",
    "        self.df_cont['labels'] = cuts.astype(str)\n",
    "        self.df_cont['label'] = cuts_show.astype(str)\n",
    "        self.df = self.df_sp_values.append(self.df_cont)\n",
    "\n",
    "        c_bins = pd.DataFrame(\n",
    "            {\"bins\": opt_bins, \"labels\": np.arange(len(opt_bins)).astype(str),\n",
    "             'label': cuts_show.cat.categories})\n",
    "        opt_bins = pd.concat([self.d_bins, c_bins])\n",
    "        self.bins = opt_bins\n",
    "        self.c_bins = opt_bins\n",
    "        return self.df, opt_bins\n",
    "\n",
    "    def optimize(self, criterion='gini', fix_depth=None, max_depth=None, cv=3,\n",
    "                 scoring='roc_auc', min_samples_leaf=None):\n",
    "        \"\"\"\n",
    "        WoE bucketing optimization\n",
    "        :param criterion: binary tree split criteria\n",
    "        :param fix_depth: use tree of a fixed depth (2^fix_depth buckets)\n",
    "        :param max_depth: maximum tree depth for a optimum cross-validation search\n",
    "        :param cv: number of cv buckets\n",
    "        :param scoring: scorer for cross_val_score\n",
    "        :param min_samples_leaf: minimum number of observations in each of optimized buckets\n",
    "        :return: WoE class with optimized continuous variable split\n",
    "        \"\"\"\n",
    "\n",
    "        # for categorical feature's optimization\n",
    "        if self.type == 'd':\n",
    "            cat_f_new, bins = self.new_bin(criterion=criterion, fix_depth=fix_depth,\n",
    "                                           max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                                           cv=cv, scoring=scoring)\n",
    "            return self.fit(cat_f_new, self.df['Y'])\n",
    "\n",
    "        # for continuous feature's optimization\n",
    "        df_new, opt_bins = self.new_bin(criterion=criterion, fix_depth=fix_depth,\n",
    "                                        max_depth=max_depth, min_samples_leaf=min_samples_leaf,\n",
    "                                        cv=cv, scoring=scoring)\n",
    "        return self.fit(None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 覆盖率计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "def coverage_sigle_col(df, mi_values=[]):\n",
    "    mi_item_total = df.iloc[df.isnull().values].shape[0]\n",
    "\n",
    "    for mi_item in mi_values:\n",
    "        mi_item_total += list(df).count(mi_item)\n",
    "\n",
    "    mi_perc = mi_item_total / len(df)\n",
    "    name_coverage = 1 - mi_perc\n",
    "    return name_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "\n",
    "def coverage_calc_multiprocess(df, colnames, mi_values=[]):\n",
    "    result = []\n",
    "\n",
    "    coverage_list = Parallel(n_jobs=-1)(delayed(coverage_sigle_col)\n",
    "                                        (df[ftr], mi_values=mi_values) for ftr in colnames)\n",
    "    result = coverage_list\n",
    "\n",
    "    output = pd.DataFrame({'feature_name': colnames, 'coverage': result})\n",
    "    output = output.sort_values('coverage', ascending=False)\n",
    "    output = output.reset_index().drop('index', axis=1)\n",
    "\n",
    "    return output[['feature_name', 'coverage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "\n",
    "def coverage_calc(df, colnames, mi_values=[]):\n",
    "    result = []\n",
    "\n",
    "    for name in colnames:\n",
    "        mi_item_total = df.iloc[df[name].isnull().values].shape[0]\n",
    "\n",
    "        for mi_item in mi_values:\n",
    "            mi_item_total += list(df[name]).count(mi_item)\n",
    "\n",
    "        mi_perc = mi_item_total / len(df[name])\n",
    "        name_coverage = 1 - mi_perc\n",
    "        result.append(name_coverage)\n",
    "\n",
    "    output = pd.DataFrame({'feature_name': colnames, 'coverage': result})\n",
    "    output = output.sort_values('coverage', ascending=False)\n",
    "    output = output.reset_index().drop('index', axis=1)\n",
    "\n",
    "    return output[['feature_name', 'coverage']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算IV,不需要区分Categorical和Continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "\n",
    "def iv_calc_single_col(df, col_name, label, method=None, opt_criterion='gini',\n",
    "                       qnt_num=10, min_block_size=200, max_depth=6,\n",
    "                       min_samples_leaf=200, cv=5, scoring=\"roc_auc\",\n",
    "                       bins=None, verbose=0, plt_show=0):\n",
    "    \"\"\"\n",
    "    Element func for parallel calculating iv.\n",
    "    \"\"\"\n",
    "    if df[col_name].dtypes != object:\n",
    "        woe = WoE(qnt_num=qnt_num,  min_block_size=min_block_size,\n",
    "                  v_type='c', bins=bins, t_type='b')\n",
    "    else:\n",
    "        woe = WoE(qnt_num=qnt_num,  min_block_size=min_block_size,\n",
    "                  v_type='d', bins=bins, t_type='b')\n",
    "    woe.read_data(df[col_name], df[label])\n",
    "    woe2 = woe.optimize(criterion=opt_criterion, max_depth=max_depth,\n",
    "                        min_samples_leaf=min_samples_leaf,\n",
    "                        cv=cv, scoring=scoring) if method == 'optimize' else woe.fit(\n",
    "        df[col_name], df[label]) if method == 'quantile' else 'invalid_value'\n",
    "\n",
    "    if woe2 == 'invalid_value':\n",
    "        raise ValueError(\n",
    "            'you should choose method from quantile and optimize')\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(woe2.bins[['mean', 'bad', 'obs', 'good',\n",
    "                         'odds', 'woe', 'label', 'iv']])\n",
    "        print(\"====================================\")\n",
    "        print(\"feature {0} iv value: {1}\".format(col_name, woe2.iv))\n",
    "        print(\"====================================\")\n",
    "    if plt_show > 0:\n",
    "        woe2.plot()\n",
    "        plt.show()\n",
    "    return woe2.iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# <api>\n",
    "\n",
    "\n",
    "def iv_calc(df, colnames, label, method=None, opt_criterion='gini',\n",
    "            qnt_num=10, min_block_size=200, max_depth=6,\n",
    "            min_samples_leaf=200, cv=5, scoring=\"roc_auc\",\n",
    "            bins=None, verbose=0, plt_show=0):\n",
    "    \"\"\"\n",
    "    Calculate IV using decision tree optimize.\n",
    "    -------------------------------------------------------------------\n",
    "    This func can be used to calculate IV of both Categorical & Continuous features.\n",
    "    ScoreIV can also be calculated with this func.\n",
    "    params:\n",
    "        method: method which used to divide the sample column: quantile or optimize(\n",
    "            when use quantile method to Categorical feature, we just don't use tree method opt)\n",
    "        qnt_num & min_block_size: used for quantile method\n",
    "        opt_cirterion & max_depth & min_samples_leaf & cv & scoring：used for decision tree optimize\n",
    "        bins: a manual set list of cut point which used with quantile method\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    # df = df.copy().reset_index(drop=True)\n",
    "    if label in colnames:\n",
    "        colnames = list(colnames)\n",
    "        colnames.remove(label)\n",
    "    iv_list = Parallel(n_jobs=-1)(delayed(iv_calc_single_col)(df[[ftr, label]], col_name=ftr,\n",
    "                                                              label=label, method=method,\n",
    "                                                              opt_criterion=opt_criterion,\n",
    "                                                              qnt_num=qnt_num,\n",
    "                                                              min_block_size=min_block_size,\n",
    "                                                              max_depth=max_depth,\n",
    "                                                              min_samples_leaf=min_samples_leaf,\n",
    "                                                              cv=cv, scoring=scoring,\n",
    "                                                              bins=bins, verbose=verbose,\n",
    "                                                              plt_show=plt_show)\n",
    "                                  for ftr in colnames)\n",
    "    result = iv_list\n",
    "\n",
    "    output = pd.DataFrame({'feature_name': colnames, 'iv': result})\n",
    "    output = output.sort_values('iv', ascending=False)\n",
    "    output = output.reset_index().drop('index', axis=1)\n",
    "\n",
    "    return output[['feature_name', 'iv']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
